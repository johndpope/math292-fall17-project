{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector representations of words\n",
    "\n",
    "There are many methods one might use to build a semantic network with words as nodes and similarity scores as weights on edges. We could also threshold this weight and just keep unweighted edges that were greater than the threshold. A Bayesian approach like Latent Dirichlet Allocation would build a semantic network with weights being probabilities of relatedness. In this vector space model (VSM) of semantics, similarity is given by the cosine of the angle between two vector representations of words, $\\mathbf{x}$ and $\\mathbf{y}$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\cos \\theta = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\lVert x \\rVert \\lVert y \\rVert}\n",
    "\\end{equation}\n",
    "\n",
    "In this notebook, I'll use TensorFlow to build and query a semantic network by following this tutorial: https://www.tensorflow.org/versions/r0.12/tutorials/word2vec/\n",
    "\n",
    "In particular, I'm taking all the code from the file `word2vec_basic` that was being run as a script and putting it here, or I have rolled it in to other helper functions that I import. I tried to put the most relevant lines here so it was clear what the essential elements are. In the script `word2vec_basic.py`, I have put all these into a function called `run_word2vec()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_basic import (\n",
    "    maybe_download, read_data, build_dataset, generate_batch, run_word2vec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "vocabulary = read_data('text8.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    vocabulary, vocabulary_size\n",
    ")\n",
    "print(type(data))\n",
    "del vocabulary\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "batch, labels = generate_batch(data, count, dictionary, reverse_dictionary, batch_size=20, num_skips=2, skip_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3081 originated -> 12 as\n",
      "3081 originated -> 5234 anarchism\n",
      "12 as -> 3081 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "2 of -> 3134 abuse\n",
      "2 of -> 195 term\n",
      "3134 abuse -> 46 first\n",
      "3134 abuse -> 2 of\n",
      "46 first -> 59 used\n",
      "46 first -> 3134 abuse\n",
      "59 used -> 46 first\n",
      "59 used -> 156 against\n",
      "156 against -> 128 early\n",
      "156 against -> 59 used\n",
      "128 early -> 156 against\n",
      "128 early -> 742 working\n"
     ]
    }
   ],
   "source": [
    "# arrows indicate \"in context of\"\n",
    "# Although BATCH_SIZE is 20, there are only 10 focal words (what is \n",
    "# correct tech term for these?), each with a window of one word on each\n",
    "# side.\n",
    "for i in range(BATCH_SIZE):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "          reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  280.573486328\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: killed, aa, audi, pizzicato, seclusion, illustrate, reminds, reworked,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: epr, valleys, sofa, conjugal, bornu, chainmail, rough, verdict,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: vab, bracing, eliminate, marinus, cadence, orbis, tonga, impressionists,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: twickenham, subtleties, converter, barron, vance, intellectualism, jihad, clones,\n",
      "Average loss at step  2000 :  96.6144672971\n",
      "Average loss at step  4000 :  39.976215231\n",
      "Average loss at step  6000 :  23.0032619903\n",
      "Average loss at step  8000 :  14.8596659407\n",
      "Average loss at step  10000 :  9.52925004646\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: used, working, describe, of, taken, class, label, radicals,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: a, has, label, still, used, the, it, also,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: of, working, to, label, has, UNK, by, a,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: label, used, describe, also, the, taken, organization, class,\n",
      "Average loss at step  12000 :  6.77193560472\n",
      "Average loss at step  14000 :  4.75414046255\n",
      "Average loss at step  16000 :  3.4476078895\n",
      "Average loss at step  18000 :  2.6478119207\n",
      "Average loss at step  20000 :  2.12954587975\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: used, working, taken, describe, class, of, label, UNK,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: has, a, label, still, valleys, also, it, the,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: the, of, a, as, working, by, been, UNK,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: used, label, describe, also, the, class, taken, of,\n",
      "Average loss at step  22000 :  1.88056702644\n",
      "Average loss at step  24000 :  1.53714857024\n",
      "Average loss at step  26000 :  1.3591943458\n",
      "Average loss at step  28000 :  1.24687573466\n",
      "Average loss at step  30000 :  1.17637198904\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: used, taken, working, describe, class, of, label, the,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: a, has, label, still, valleys, the, also, it,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: the, of, a, as, by, working, to, has,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: used, describe, label, also, the, of, class, organization,\n",
      "Average loss at step  32000 :  1.06000243351\n",
      "Average loss at step  34000 :  1.09860159323\n",
      "Average loss at step  36000 :  1.02788523731\n",
      "Average loss at step  38000 :  1.0001754325\n",
      "Average loss at step  40000 :  1.0242155849\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: used, taken, working, the, class, of, describe, label,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: a, has, label, valleys, still, the, also, of,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: the, of, a, to, working, as, by, term,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: used, describe, label, also, the, of, society, to,\n",
      "Average loss at step  42000 :  0.965066072464\n",
      "Average loss at step  44000 :  0.96564273271\n",
      "Average loss at step  46000 :  0.947993963212\n",
      "Average loss at step  48000 :  0.958261501998\n",
      "Average loss at step  50000 :  0.952448686928\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: used, taken, working, of, class, the, label, has,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: a, has, label, valleys, still, the, of, act,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: of, the, a, working, to, as, by, been,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: used, describe, also, label, society, of, the, to,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52000 :  0.95212925604\n",
      "Average loss at step  54000 :  0.950097527623\n",
      "Average loss at step  56000 :  0.938702977329\n",
      "Average loss at step  58000 :  0.94018282935\n",
      "Average loss at step  60000 :  0.94024964264\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: taken, used, working, of, the, class, a, has,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: a, has, valleys, label, still, of, the, act,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: a, of, the, to, working, by, been, as,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: used, describe, society, also, of, to, the, label,\n",
      "Average loss at step  62000 :  0.949604044765\n",
      "Average loss at step  64000 :  0.940227025449\n",
      "Average loss at step  66000 :  0.941886028528\n",
      "Average loss at step  68000 :  0.937973680019\n",
      "Average loss at step  70000 :  0.944919203073\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: taken, used, working, the, of, label, class, a,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: a, valleys, has, label, the, of, still, act,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: the, a, of, to, as, working, by, been,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: used, describe, society, to, also, of, the, label,\n",
      "Average loss at step  72000 :  0.94064714095\n",
      "Average loss at step  74000 :  0.94169673115\n",
      "Average loss at step  76000 :  0.941187797368\n",
      "Average loss at step  78000 :  0.931199508041\n",
      "Average loss at step  80000 :  0.93076574108\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: taken, used, working, of, the, has, class, a,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: a, valleys, has, label, of, act, the, still,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: the, of, a, to, as, working, by, been,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: used, society, describe, to, of, the, also, as,\n",
      "Average loss at step  82000 :  0.936798666239\n",
      "Average loss at step  84000 :  0.939973784402\n",
      "Average loss at step  86000 :  0.937921769798\n",
      "Average loss at step  88000 :  0.936820984423\n",
      "Average loss at step  90000 :  0.938211711407\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: taken, used, working, a, the, label, describe, of,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: a, the, of, valleys, label, has, act, still,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: a, the, of, to, as, working, by, self,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: society, describe, used, to, of, also, a, the,\n",
      "Average loss at step  92000 :  0.936403160602\n",
      "Average loss at step  94000 :  0.940159180194\n",
      "Average loss at step  96000 :  0.934230974644\n",
      "Average loss at step  98000 :  0.935149808496\n",
      "Average loss at step  100000 :  0.936177789927\n",
      "Nearest to known: generalship, bk, karim, apologists, pangloss, parks, jacobian, raining,\n",
      "Nearest to often: strongly, coa, ironworks, permit, basketball, dieu, integer, expressway,\n",
      "Nearest to six: amiga, ordo, adaptations, multiracial, believer, riffing, roxy, nawab,\n",
      "Nearest to it: taken, used, working, a, label, class, the, of,\n",
      "Nearest to american: cried, circuit, showings, paullus, flush, playoff, foresee, hindustani,\n",
      "Nearest to such: pores, eyeglasses, yuan, unveils, boycott, imprisoned, heard, unfettered,\n",
      "Nearest to is: a, of, valleys, the, act, label, has, still,\n",
      "Nearest to an: eff, erm, simultaneity, bag, leopard, exchanged, deconstruction, dal,\n",
      "Nearest to d: consequently, implemented, nightmarish, laziness, chipmunk, lucie, viewing, cnt,\n",
      "Nearest to state: headers, ago, sfg, naxos, naci, oblivion, ataxia, gul,\n",
      "Nearest to two: wt, cpu, electorate, tarkovsky, mina, continuity, mccartney, encyclical,\n",
      "Nearest to used: a, to, the, of, as, by, describe, working,\n",
      "Nearest to history: anachronism, enactment, unwritten, closes, interpolated, heaviside, urbino, randi,\n",
      "Nearest to states: direction, might, nursed, ssel, geneticist, melisende, eia, polyn,\n",
      "Nearest to th: chennai, diazepam, empower, austen, crazy, praised, amplitude, racism,\n",
      "Nearest to been: society, used, describe, to, of, also, as, a,\n"
     ]
    }
   ],
   "source": [
    "run_word2vec(data, count, dictionary, reverse_dictionary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, great, so we broke the script into slightly smaller pieces. But this is a little too much, still. I'd like to have this be like Bhat's lecture ipynbs, or the other tutorial nbs, where different elements of the graph are computed at different times. Before this, I want to build out the project infrastructure a bit more.\n",
    "\n",
    "Here I'm going to try to extract the constituent parts for building and optimizing the network. Skipping the data (down)load part.\n",
    "\n",
    "Somehow the embedding matrix is represented as a graph. This still does not make sense to me yet, so that's one of the goals I have at this point. \n",
    "\n",
    "\n",
    "### Part 1: Build a skip-gram model\n",
    "\n",
    "The skip-gram model predicts the context of $2m$ words occurring in a window of width $m$. As explained in the comments of word2vec_basic.py, $m$ is called the `skip_window`. Another parameter does something I don't understand yet, but is related: `num_skips`.\n",
    "\n",
    "I'm trying to read through three of the source papers, [Mikolov, et al., 2013. _Efficient Estimation of Word Representations in Vector Space_](http://arxiv.org/abs/1301.3781), [Mikolov, et al., 2013. _Distributed Representations of Words and Phrases and their Compositionality_](http://arxiv.org/abs/1310.4546), and [Gutman and Aapo, 2010. _Noise-contrastive estimation: A new estimation principle for unnormalized statistical models_](http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf). I'll add notes from these as I'm able.\n",
    "\n",
    "In the code, the first step in `run_word2vec`, which I composed from parts of the script `word2vec_basic.py`, is to create the `valid_examples` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some helper functions to identify different blocks\n",
    "# and simplify the code that sets up the computations\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# function for creating training inputs and labels\n",
    "def make_training_placeholders(batch_size):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    \n",
    "    return train_inputs, train_labels\n",
    "\n",
    "\n",
    "# create loss function; requires embeddings be created first and passed\n",
    "def make_loss_op(embeddings, vocabulary_size, \n",
    "                 embedding_size, train_inputs):\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size))\n",
    "    )\n",
    "    \n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weights,\n",
    "            biases=nce_biases,\n",
    "            labels=train_labels,\n",
    "            inputs=embed,\n",
    "            num_sampled=num_sampled,\n",
    "            num_classes=vocabulary_size\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "# now construct loss and optimizer, which will be run in same\n",
    "# session.run call in another cell\n",
    "\n",
    "# need to define some important variables\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "num_sampled = 64\n",
    "\n",
    "# let's reload the data as a reminder; don't run this multiple \n",
    "# times like a dope\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "vocabulary = read_data(filename)\n",
    "vocabulary_size = 50000\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    vocabulary, vocabulary_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "    )\n",
    "\n",
    "    train_inputs, train_labels = make_training_placeholders(batch_size)\n",
    "\n",
    "    loss = make_loss_op(embeddings, vocabulary_size, \n",
    "                        embedding_size, train_inputs)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "\n",
    "    # just used to display model accuracy; doesn't affect calculation\n",
    "    valid_size = 4\n",
    "    valid_window = 100\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # cosine similarity between minimatch examples and embeddings\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset\n",
    "    )\n",
    "    # why should this be similarity? write it out below\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True\n",
    "    )\n",
    "\n",
    "    # add variable initializer for the variables we've just defined \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Train a skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper functions to print interesting metrics\n",
    "def more_frequent_printout(step, average_loss):\n",
    "    if step > 0:\n",
    "        average_loss /= 2000\n",
    "    print('Average loss at step {}:'.format(step), average_loss)\n",
    "    average_loss = 0\n",
    "    \n",
    "    \n",
    "def less_frequent_printout(similarity, valid_size, reverse_dictionary,\n",
    "                           valid_examples, num_nearest=8):\n",
    "    # Run calculation of similarity at this timestep.\n",
    "    # expensive b/c similarity involves mat-mat mult of \n",
    "    # embedding matrix \n",
    "    sim = similarity.eval()\n",
    "    # now iterate through all `valid_size` words and see \n",
    "    # nearest `num_nearest` neighbors\n",
    "    for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = num_nearest\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        \n",
    "        log_str = 'Nearest to {}:'.format(valid_word)\n",
    "        \n",
    "        for k in range(top_k):\n",
    "            close_word = reverse_dictionary[nearest[k]]\n",
    "            log_str = '{} {},'.format(log_str, close_word)\n",
    "        \n",
    "        print(log_str[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 273.19909668\n",
      "Average loss at step 1000: 62.524368\n",
      "Average loss at step 2000: 96.6401708021\n",
      "Average loss at step 3000: 119.657098875\n",
      "Nearest to i: relocated, ean, wearer, fingered, hmos, pcm, roy, hobbyist, competent, sident\n",
      "Nearest to only: sca, implantation, threat, bandit, starr, rationing, uml, kasparov, end, camels\n",
      "Nearest to time: destroyer, mascarenes, pressure, vig, veneer, turns, ribs, tiberian, subscribers, methionine\n",
      "Nearest to with: cuauht, cro, skaro, shams, nmt, snare, testify, probe, prevail, voter\n",
      "Average loss at step 4000: 136.884147372\n",
      "Average loss at step 5000: 150.085542099\n",
      "Average loss at step 6000: 160.310264404\n",
      "Nearest to i: relocated, ean, wearer, fingered, hmos, pcm, roy, hobbyist, competent, sident\n",
      "Nearest to only: sca, implantation, threat, bandit, starr, rationing, uml, kasparov, end, camels\n",
      "Nearest to time: destroyer, mascarenes, pressure, vig, veneer, turns, ribs, tiberian, subscribers, methionine\n",
      "Nearest to with: cuauht, cro, skaro, shams, nmt, snare, testify, probe, prevail, voter\n",
      "Average loss at step 7000: 167.959334838\n",
      "Average loss at step 8000: 174.215889361\n",
      "Average loss at step 9000: 179.475792323\n",
      "Nearest to i: relocated, ean, wearer, fingered, hmos, pcm, roy, hobbyist, competent, sident\n",
      "Nearest to only: sca, implantation, threat, bandit, starr, rationing, uml, kasparov, end, camels\n",
      "Nearest to time: destroyer, mascarenes, pressure, vig, veneer, turns, ribs, tiberian, subscribers, methionine\n",
      "Nearest to with: cuauht, cro, skaro, shams, nmt, snare, testify, probe, prevail, voter\n",
      "Average loss at step 10000: 183.958812666\n"
     ]
    }
   ],
   "source": [
    "# code that sets up a session and trains the network within that session\n",
    "\n",
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    init.run()\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        # select random batches of contexts to train on\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            data, count, dictionary, reverse_dictionary,\n",
    "            batch_size, num_skips, skip_window\n",
    "        )\n",
    "        # each batch has a different feed_dict\n",
    "        feed_dict = {train_inputs: batch_inputs, \n",
    "                     train_labels: batch_labels}\n",
    "        \n",
    "        # run the session for the current batch\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            more_frequent_printout(step, average_loss)\n",
    "        \n",
    "        if step % 3000 == 0 and step > 0:\n",
    "            less_frequent_printout(similarity, valid_size,\n",
    "                                   reverse_dictionary, valid_examples,\n",
    "                                   num_nearest=10)\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output embeddings have dimension V x D, where V is the size of\n",
    "# the vocabulary and D is the embedding dimension \n",
    "# (50000 x 128 in our example)\n",
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Word embedding directory in the tensorflow/models GitHub repo](https://github.com/tensorflow/models/tree/master/tutorials/embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
